{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f363fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9336766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\achil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\achil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pathlib import Path\n",
    "import sys# make data_path pathlib-friendly and resolve relative to this file\n",
    "code_path = Path(\"../code\")\n",
    "# if not code_path.is_absolute():\n",
    "#     # resolve relative to the repository/code file location (works on Windows)\n",
    "#     code_path = (Path(__file__).resolve().parent / code_path).resolve()\n",
    "\n",
    "sys.path.append(str(code_path))\n",
    "from text_preprocessing import preprocess_sentence\n",
    "\n",
    "\n",
    "MODEL = os.getenv(\"LMSTUDIO_MODEL\")\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"LMSTUDIO_BASE_URL\"),\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "def get_embedding(text, model=\"model-identifier\"):\n",
    "   text = text.replace(\"\\n\", \" \")   \n",
    "   embedding = client.embeddings.create(input=[text], model=MODEL).data[0].embedding\n",
    "   return np.array(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cec9ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781ea3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = {'code' : [], 'symptome' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4b07b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatched codes: 98\n",
      "number of symptoms before deduplication: 1554\n",
      "number of symptoms after deduplication: 807\n"
     ]
    }
   ],
   "source": [
    "for i in data.index : \n",
    "    text = data.description[i].split('..')\n",
    "    for t in text : \n",
    "        t = preprocess_sentence(t)\n",
    "        dataframe['code'].append(data.code[i])\n",
    "        dataframe['symptome'].append(t)\n",
    "\n",
    "database = pd.DataFrame(dataframe)\n",
    "database[\"code\"] = database[\"code\"].str.split('_').str[0]\n",
    "\n",
    "# prepare disorder names\n",
    "raw_data_file = \"../data/33clusters30PCs_withNames.csv\"\n",
    "raw_data = pd.read_csv(raw_data_file, index_col=None)\n",
    " # Split pipe-separated values in 'code' into separate rows, repeating other columns,\n",
    "raw_data['code'] = raw_data['code'].astype(str)\n",
    "raw_data['code_list'] = raw_data['code'].str.split('|')\n",
    "expanded = raw_data.explode('code_list').copy()\n",
    "expanded['code'] = expanded['code_list'].str.strip()\n",
    "expanded = expanded.drop(columns=['code_list']).reset_index(drop=True)\n",
    "# now `expanded` has one row per code; assign back if you want to replace raw_data,\n",
    "raw_data = expanded\n",
    "\n",
    "raw_data['code_key'] = raw_data['code'].astype(str).str.extract(r'(^[A-Z]\\d{2}(?:\\.\\d+)?)', expand=False)\n",
    "\n",
    "# build mapping from code_key -> diagnosis (take first if duplicates)\n",
    "mapping = raw_data.groupby('code_key')['diagnosis'].first().to_dict()\n",
    "\n",
    "# map to database (database['code'] should already be cleaned like \"F70\" or \"F60.2\")\n",
    "database['disorder'] = database['code'].map(mapping)\n",
    "database = database[['code', 'disorder', 'symptome']]\n",
    "# optional: see how many didn't match\n",
    "print(f\"unmatched codes: {database['disorder'].isna().sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"number of symptoms before deduplication: {len(database)}\")\n",
    "# drop duplicates to avoid redundant symptoms\n",
    "database = database.drop_duplicates(subset=[\"symptome\"]).reset_index(drop=True)\n",
    "print(f\"number of symptoms after deduplication: {len(database)}\")\n",
    "database.to_csv('../data/datalong.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b68a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_symptoms = pd.read_csv('../data/datalong.csv', index_col=0)\n",
    "symptoms_embeds = {\"disorder\" : [], \"symptome\" : [], \"embedding\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f420802",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list_symptoms.index : \n",
    "    symptoms_embeds['disorder'].append(list_symptoms.disorder[i])\n",
    "    symptoms_embeds['symptome'].append(list_symptoms.symptome[i])\n",
    "    symptoms_embeds['embedding'].append(get_embedding(list_symptoms.symptome[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc29cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms_embeddings = pd.DataFrame(symptoms_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ede3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/symptomes_embedding.pkl', 'wb') as fp:\n",
    "        pickle.dump(symptoms_embeddings, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeeec5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b155d3de",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sent2vec' has no attribute 'Sent2vecModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msent2vec\u001b[39;00m\n\u001b[32m      3\u001b[39m model_path = \u001b[33m\"\u001b[39m\u001b[33m../models/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43msent2vec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSent2vecModel\u001b[49m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m     model.load_model(model_path)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'sent2vec' has no attribute 'Sent2vecModel'"
     ]
    }
   ],
   "source": [
    "# repeat using BioSentVec model\n",
    "import sent2vec\n",
    "model_path = \"../models/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\n",
    "model = sent2vec.Sent2vecModel()\n",
    "try:\n",
    "    model.load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('model successfully loaded')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autodiag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
